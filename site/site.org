#+OPTIONS: ':nil -:nil ^:{} num:t toc:nil
#+AUTHOR: Hiroyuki Yamada
#+CREATOR: Emacs 26.1 (Org mode 9.1.14 + ox-hugo)
#+HUGO_WITH_LOCALE:
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: .
#+HUGO_BUNDLE:
#+HUGO_BASE_DIR: ./
#+HUGO_CODE_FENCE:
#+HUGO_USE_CODE_FOR_KBD:
#+HUGO_PREFER_HYPHEN_IN_TAGS:
#+HUGO_ALLOW_SPACES_IN_TAGS:
#+HUGO_AUTO_SET_LASTMOD:
#+HUGO_CUSTOM_FRONT_MATTER:
#+HUGO_BLACKFRIDAY:
#+HUGO_FRONT_MATTER_KEY_REPLACE:
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_PAIRED_SHORTCODES:
#+HUGO_PANDOC_CITATIONS:
#+BIBLIOGRAPHY:
#+HUGO_ALIASES:
#+HUGO_AUDIO:
#+DATE: <2019-02-10 Sun>
#+DESCRIPTION:
#+HUGO_DRAFT:
#+HUGO_EXPIRYDATE:
#+HUGO_HEADLESS:
#+HUGO_IMAGES:
#+HUGO_ISCJKLANGUAGE:
#+KEYWORDS:
#+HUGO_LAYOUT:
#+HUGO_LASTMOD:
#+HUGO_LINKTITLE:
#+HUGO_LOCALE:
#+HUGO_MARKUP:
#+HUGO_MENU:
#+HUGO_MENU_OVERRIDE:
#+HUGO_OUTPUTS:
#+HUGO_PUBLISHDATE:
#+HUGO_SERIES:
#+HUGO_SLUG:
#+HUGO_TAGS:
#+HUGO_CATEGORIES:
#+HUGO_RESOURCES:
#+HUGO_TYPE:
#+HUGO_URL:
#+HUGO_VIDEOS:
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

* Installation
:PROPERTIES:
:EXPORT_HUGO_SECTION*: installation
:END:


* Implemented Algorithms
:PROPERTIES:
:EXPORT_HUGO_SECTION*: algorithms
:END:

** DONE Probabilistic Backpropagation
CLOSED: [2020-09-22 Tue 06:07]
:PROPERTIES:
:EXPORT_FILE_NAME: pbp
:END:

*** Overview
Probabilistic Backpropagation (PBP) is an algorithm to focus
scalability. PBP uses a fully connected neural network with its
weights and biases obeying Gaussian distributions, i.e.

\[ f(\thinspace \cdot \thinspace ; W)\quad \text{where}\quad W_{ij}^{(l)} \sim \mathcal{N}(m_{ij}^{(l)}, v_{ij}^{(l)}) \text{.}\]

The means \( m_{ij}^{(l)} \) and variances \( v_{ij}^{(l)} \) of the
network are trained parameters. Additionally, observations (\( x \)
and \( y \)) are affected by Gaussian noise whose reciprocal of variance
obeys Gamma distribution, i.e.

\[ y = \mathcal{N}(f(x;W), \gamma^{-1})\quad \text{where}\quad \gamma \sim \mathrm{Gamma}(\alpha, \beta) \text{.}\]

The shape parameter \( \alpha \) and rate parameter \( \beta \) are
trained parameters.

PBP utilizes assumed density filtering (ADF), which does not require
any expectations but requires partition function
\(\mathcal{Z}(\thinspace \cdot \thinspace , \thinspace \cdot \thinspace )\)
for normalization.  ADF updates for Gaussian and Gamma distributions
are followings;

\begin{align}
m^{\text{new}} &= m + v \frac{\partial \log Z}{\partial m} \\
v^{\text{new}} &= v - v^2 \left [ \left ( \frac{\partial \log Z}{\partial m} \right )^2 -2 \frac{\partial \log Z}{\partial v}\right ] \\
\alpha^{\text{new}} &= \left [ \frac{ZZ_2}{Z_1^2}\frac{\alpha +1}{\alpha}  - 1.0 \right ] ^{-1}\\
\beta^{\text{new}} &= \left [ \frac{Z_2}{Z_1}\frac{\alpha+1}{\beta}  - \frac{Z_1}{Z}\frac{\alpha}{\beta} \right ] ^{-1}
\end{align}

where \( Z_1 = \mathcal{Z}(\alpha+1,\beta)\) and
\( Z_2 = \mathcal{Z}(\alpha+2,\beta) \) when \( Z = \mathcal{Z}(\alpha,\beta)\).

One of the key concepts in PBP is propagation of means \( m^{z_l} \)
and variances \( v^{z_l} \) at hidden layers from its input layer to
its output layer in order to calculate the partition function.  Each
unit at hidden layers is assumed to follow Gaussian distribution. We
also know the means \( M_l \) and variances \( V_l \) of the network
weights and biases, so that we can calculate means \( m^{a_l} \) and
variances \( v^{a_l} \) of affine transformation from the previous
layer.

\begin{align}
m^{a_l} =& \frac{M_l m^{z_{l-1}}}{\sqrt{N_{l-1}+1}}\\
v^{a_l} =& \frac{\left [ (M_l \circ M_l) v^{z_{l-1}} + V_l (m^{z_{l-1}} \circ m^{z_{l-1}}) + V_l v^{z_{l-1}} \right ]}{N_{l-1}+1}
\end{align}
The operation \( \circ \) denotes element-wise multiplication.

PBP uses ReLU as activation at hidden layers, the means \( m^{b_l} \)
and variances \( v^{b_l} \) of activated units are caculated as
follows;

\begin{align}
\alpha _i =& \frac{m_i ^{a_l}}{\sqrt{v _i ^{a_l}}} \\
\gamma _i =&
\begin{cases}
\frac{\phi (-\alpha _i)}{\Phi (\alpha )} \\
- \alpha _i - \alpha _i ^{-1} + 2\alpha ^{-3}\ \text{for}\ \alpha _i < -30
\end{cases} \\
v _i ^{\prime} =& m_i ^{a_l} + \sqrt{v _i ^{a_l}} \gamma _i \\
m_i ^{b_l} &= \Phi (\alpha _i)v_i ^{\prime} \\
v_i ^{b_l} &= m_i^{b_l}v_i^{\prime}\Phi (-\alpha _i) + \Phi (\alpha _i)v_i^{a_l}(1-\gamma _i(\gamma _i + \alpha _i))
\end{align}

Finally, means and variances of the hidden layers is calculated by
following concatenation;

\begin{align}
m ^{z_l} =& [m ^{b_l}; 1]\\
v ^{z_l} =& [v ^{b_l}; 0]
\end{align}

*** Usage in b4tf
#+begin_src python
import b4tf

x = # Observed Input
y = # Observed Output


# Create PBP with
#   Input: (1,)  default
#   Hidden Layers: 50 and 50
#   Output: 1
pbp = b4tf.models.PBP([50,50,1],input_shape=(1,))

# Learn data
# x, y should be normalized (mean=0,std=1) beforehand.
pbp.fit(x,y)


# Call deterministic
#    Sample weights and biases from approximated posterior
_y = pbp(x)


# Predict output
#    Outputs are mean and variance
m, v = pbp.predict(x)
#+end_src

*** References
- Paper :: [[https://arxiv.org/abs/1502.05336][J. M. HernÃ¡ndez-Lobato and R. P. Adams, "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks" (2015) arXiv:1502.05336]]
- Code :: [[https://github.com/HIPS/Probabilistic-Backpropagation][HIPS/Probabilistic-Backpropagation]]


* Contributing
:PROPERTIES:
:EXPORT_HUGO_SECTION*: contributing
:END:

** DONE Step by Step Merge Request
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: merge_request
:END:

The first step of coding contribution is to fork b4tf on GitLab.com.

The detail steps for fork is described at [[https://docs.gitlab.com/ee/gitlab-basics/fork-project.html][official document]].

After fork b4tf on the web, you can clone repository to your local
machine and set original b4tf as "upstream" by

#+begin_src shell
git clone https://gitlab.com/<Your GitLab Account>/b4tf.git
cd b4tf
git remote add upstream https://gitlab.com/ymd_h/b4tf.git
#+end_src

To make "master" branch clean, you need to create new branch before you edit.

#+begin_src shell
git checkout -b <New Branch Name> master
#+end_src

This process is necessay because "master" and other original branches
might progress during your working.


From here, you can edit codes and make commit as usual.


After finish your work, you must recheck original b4tf and ensure
there is no cnflict.

#+begin_src shell
git pull upstream master
git checkout <Your Branch Name>
git merge master # Fix confliction here!
#+end_src


If everything is fine, you push to your b4tf.

#+begin_src shell
git push origin <Your Branch Name>
#+end_src

Merge request can be created from the web, the detail is described at
[[https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html][official document]].


There is [[https://stackoverflow.com/a/14681796][a good explanation]] for making good Pull Request (merge
request equivalent on GitHub.com)

* DONE Examples
CLOSED: [2020-02-15 Sat 09:23]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: examples
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 800
:END:


* DONE Misc
CLOSED: [2020-01-17 Fri 22:31]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: misc
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 999
:END:

In this section, b4tf related miscellaneous information are described.

- [[https://ymd_h.gitlab.io/b4tf/misc/links/][Links]]
- [[https://ymd_h.gitlab.io/b4tf/misc/lisence/][License]]

* DONE FAQ
CLOSED: [2020-06-06 Sat 13:50]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION*: faq
:EXPORT_HUGO_WEIGHT: 900
:END:
